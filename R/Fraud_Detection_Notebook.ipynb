{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection\n",
    "\n",
    "This notebook takes advantage of the power of SQL Server and RevoScaleR (Microsoft R Server). The tables are all stored in a SQL Server, and most of the computations are done by loading chunks of data in-memory instead of the whole dataset.\n",
    "\n",
    "It does the following: \n",
    "\n",
    " * **Step 0: Packages, Compute Contexts and Database Creation**\n",
    " * **Step 1: Creating the Tagged Data**\n",
    " * **Step 2: Splitting and Data Cleaning of the training set**\n",
    " * **Step 3: Feature Engineering on the training set**\n",
    " * **Step 4: Training, Preprocessing and Feature Engineering on the test set, Scoring and Evalutating the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Packages, Compute Contexts and Database Creation\n",
    "\n",
    "#### In this step, we set up the connection string to access a SQL Server Database we create and load the necessary packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WARNING.\n",
    "# We recommend not using Internet Explorer as it does not support plotting, and may crash your session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT DATA SETS: point to the correct path.  \n",
    "Untagged_Transactions <- \"C:/Solutions/Fraud/Data/Untagged_Transactions.csv\"\n",
    "Account_Info <- \"C:/Solutions/Fraud/Data/Account_Info.csv\"\n",
    "Fraud_Transactions <- \"C:/Solutions/Fraud/Data/Fraud_Transactions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages.\n",
    "library(RevoScaleR)\n",
    "library(\"MicrosoftML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the connection string. Specify:\n",
    "## Database name. If it already exists, tables will be overwritten. If not, it will be created.\n",
    "## Server name. If conecting remotely to the DSVM, the full DNS address should be used with the port number 1433 (which should be enabled) \n",
    "\n",
    "db_name <- \"FraudR\"\n",
    "server <- \"localhost\"\n",
    "\n",
    "connection_string <- sprintf(\"Driver=SQL Server;Server=%s;Database=%s;Trusted_Connection=True\", server, db_name)\n",
    "\n",
    "print(\"Connection String Written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database if not already existing. \n",
    "\n",
    "## Open an Odbc connection with SQL Server master database only to create a new database with the rxExecuteSQLDDL function.\n",
    "connection_string_master <- sprintf(\"Driver=SQL Server;Server=%s;Database=master;Trusted_Connection=True\", server)\n",
    "outOdbcDS_master <- RxOdbcData(table = \"Default_Master\", connectionString = connection_string_master)                         \n",
    "rxOpen(outOdbcDS_master, \"w\")\n",
    "\n",
    "## Create database if not already existing. \n",
    "query <- sprintf( \"if not exists(SELECT * FROM sys.databases WHERE name = '%s') CREATE DATABASE %s;\", db_name, db_name)\n",
    "rxExecuteSQLDDL(outOdbcDS_master, sSQLString = query)\n",
    "\n",
    "## Close Obdc connection to master database. \n",
    "rxClose(outOdbcDS_master)\n",
    "\n",
    "print(\"Database created if not already existing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SQL Compute Context.\n",
    "sql <- RxInSqlServer(connectionString = connection_string)\n",
    "\n",
    "# Open a connection with SQL Server to be able to write queries with the rxExecuteSQLDDL function in the new database.\n",
    "outOdbcDS <- RxOdbcData(table = \"Default\", connectionString = connection_string)\n",
    "rxOpen(outOdbcDS, \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The function below can be used to get the top n rows of a table stored on SQL Server. \n",
    "#### You can execute this cell throughout your progress by removing the comment \"#\", and inputting:\n",
    "#### - the table name.\n",
    "#### - the number of rows you want to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_head <- function(table_name, n_rows){\n",
    "   table_sql <- RxSqlServerData(sqlQuery = sprintf(\"SELECT TOP(%s) * FROM %s\", n_rows, table_name), connectionString = connection_string)\n",
    "   table <- rxImport(table_sql)\n",
    "   print(table)\n",
    "}\n",
    "\n",
    "# table_name <- \"insert_table_name\"\n",
    "# n_rows <- 10\n",
    "# display_head(table_name, n_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating the Tagged Data\n",
    "\n",
    "In this step, we:\n",
    "\n",
    "**1.** Upload the 3 data sets Untagged_Transactions, Account_Info and Fraud_Transactions from disk to SQL Server\n",
    "\n",
    "**2.** Create the transactionDateTime variable based on transactionDate and transactionTime.\n",
    "\n",
    "**3.** Merge the two tables Untagged_Transaction ad Account_Info.\n",
    "\n",
    "**4.** Remove duplicates from the 2 tables. \n",
    "\n",
    "**5.** Merge the 2 tables and create the label.\n",
    "\n",
    "**Input:** 3 Data Tables: Untagged_Transactions, Account_Info and Fraud_Transactions.\n",
    "\n",
    "**Output:** Tagged data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the compute context to Local. \n",
    "rxSetComputeContext('local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Upload the data set to SQL.\n",
    "\n",
    "## Specify the desired column types. \n",
    "## Character and Factor are converted to varchar(255) in SQL Server. \n",
    "column_types_untagged <- c(transactionID = \"character\",\n",
    "                           accountID = \"character\",\n",
    "                           transactionAmountUSD = \"character\",\n",
    "                           transactionAmount = \"character\",\n",
    "                           transactionCurrencyCode = \"character\",\n",
    "                           transactionCurrencyConversionRate = \"character\",\n",
    "                           transactionDate = \"character\",\n",
    "                           transactionTime = \"character\",\n",
    "                           localHour = \"character\",\n",
    "                           transactionScenario = \"character\",\n",
    "                           transactionType = \"character\",\n",
    "                           transactionMethod = \"character\",\n",
    "                           transactionDeviceType = \"character\",\n",
    "                           transactionDeviceId = \"character\",\n",
    "                           transactionIPaddress = \"character\",\n",
    "                           ipState = \"character\",\n",
    "                           ipPostcode = \"character\",\n",
    "                           ipCountryCode = \"character\",\n",
    "                           isProxyIP = \"character\",\n",
    "                           browserType = \"character\",\n",
    "                           browserLanguage = \"character\",\n",
    "                           paymentInstrumentType = \"character\",\n",
    "                           cardType = \"character\",\n",
    "                           cardNumberInputMethod = \"character\",\n",
    "                           paymentInstrumentID = \"character\",\n",
    "                           paymentBillingAddress = \"character\",\n",
    "                           paymentBillingPostalCode = \"character\",\n",
    "                           paymentBillingState = \"character\",\n",
    "                           paymentBillingCountryCode = \"character\",\n",
    "                           paymentBillingName = \"character\",\n",
    "                           shippingAddress = \"character\",\n",
    "                           shippingPostalCode = \"character\",\n",
    "                           shippingCity = \"character\",\n",
    "                           shippingState = \"character\",\n",
    "                           shippingCountry = \"character\",\n",
    "                           cvvVerifyResult = \"character\",\n",
    "                           responseCode = \"character\",\n",
    "                           digitalItemCount = \"character\",\n",
    "                           physicalItemCount = \"character\",\n",
    "                           purchaseProductType = \"character\")\n",
    "\n",
    "column_types_account <- c(accountID = \"character\",\n",
    "                          transactionDate = \"character\",\n",
    "                          transactionTime = \"character\",  \n",
    "                          accountOwnerName = \"character\",\n",
    "                          accountAddress = \"character\",\n",
    "                          accountPostalCode = \"character\",\n",
    "                          accountCity = \"character\",\n",
    "                          accountState = \"character\",\n",
    "                          accountCountry = \"character\",\n",
    "                          accountOpenDate = \"character\",\n",
    "                          accountAge = \"character\",\n",
    "                          isUserRegistered = \"character\",\n",
    "                          paymentInstrumentAgeInAccount = \"character\",\n",
    "                          numPaymentRejects1dPerUser = \"character\")\n",
    "\n",
    "column_types_fraud <- c(transactionID = \"character\",\n",
    "                        accountID = \"character\",\n",
    "                        transactionAmount = \"character\",\n",
    "                        transactionCurrencyCode = \"character\",\n",
    "                        transactionDate = \"character\", \n",
    "                        transactionTime = \"character\",\n",
    "                        localHour = \"character\",\n",
    "                        transactionDeviceId = \"character\",\n",
    "                        transactionIPaddress = \"character\")\n",
    "  \n",
    "## Point to the input data sets while specifying the classes.\n",
    "Untagged_Transactions_text <- RxTextData(file = Untagged_Transactions, colClasses = column_types_untagged)\n",
    "Account_Info_text <- RxTextData(file = Account_Info, colClasses = column_types_account)\n",
    "Fraud_Transactions_text <- RxTextData(file = Fraud_Transactions, colClasses = column_types_fraud)\n",
    "\n",
    "## Upload the data to SQL tables. \n",
    "## At the same time, we create transactionDateTime. This is done by:\n",
    "### converting transactionTime into a 6 digit time.\n",
    "### concatenating transactionDate and transactionTime.\n",
    "### converting it to a DateTime \"%Y%m%d %H%M%S\" format. \n",
    "\n",
    "Untagged_Transactions_sql <- RxSqlServerData(table = \"Untagged_Transactions\", connectionString = connection_string)\n",
    "Account_Info_sql <- RxSqlServerData(table = \"Account_Info\", connectionString = connection_string)\n",
    "Fraud_Transactions_sql <- RxSqlServerData(table = \"Fraud_Transactions\", connectionString = connection_string)\n",
    "\n",
    "rxDataStep(inData = Untagged_Transactions_text, outFile = Untagged_Transactions_sql, overwrite = TRUE, \n",
    "           transforms = list(\n",
    "             transactionDateTime = as.character(as.POSIXct(paste(transactionDate, sprintf(\"%06d\", as.numeric(transactionTime)), sep=\"\"), format = \"%Y%m%d %H%M%S\", tz = \"GMT\"))\n",
    "           ))\n",
    "\n",
    "rxDataStep(inData = Account_Info_text, outFile = Account_Info_sql, overwrite = TRUE, \n",
    "           transforms = list(\n",
    "             recordDateTime = as.character(as.POSIXct(paste(transactionDate, sprintf(\"%06d\", as.numeric(transactionTime)), sep=\"\"), format = \"%Y%m%d %H%M%S\", tz = \"GMT\"))\n",
    "           ))\n",
    "\n",
    "rxDataStep(inData = Fraud_Transactions_text, outFile = Fraud_Transactions_sql, overwrite = TRUE, \n",
    "           transforms = list(\n",
    "             transactionDateTime = as.character(as.POSIXct(paste(transactionDate, sprintf(\"%06d\", as.numeric(transactionTime)), sep=\"\"), format = \"%Y%m%d %H%M%S\", tz = \"GMT\"))\n",
    "           ))\n",
    "\n",
    "print(\"Data exported to SQL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the compute context to SQL. \n",
    "rxSetComputeContext(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert transactionDateTime to a datetime format in SQL Server. \n",
    "rxExecuteSQLDDL(outOdbcDS, sSQLString = paste(\"ALTER TABLE Untagged_Transactions ALTER COLUMN transactionDateTime datetime;\"\n",
    "                                              , sep=\"\"))\n",
    "\n",
    "rxExecuteSQLDDL(outOdbcDS, sSQLString = paste(\"ALTER TABLE Account_Info ALTER COLUMN recordDateTime datetime;\"\n",
    "                                              , sep=\"\"))\n",
    "\n",
    "rxExecuteSQLDDL(outOdbcDS, sSQLString = paste(\"ALTER TABLE Fraud_Transactions ALTER COLUMN transactionDateTime datetime;\"\n",
    "                                              , sep=\"\"))\n",
    "\n",
    "print(\"transactionDateTime converted to a datetime type in SQL Server.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort Account_Info in descending order of accountID, transactionDateTime. \n",
    "# Note: SQL queries are used here because the rxSort function is not available for SQL data sources.\n",
    "rxExecuteSQLDDL(outOdbcDS, sSQLString = paste(\"DROP TABLE if exists Account_Info_Sort;\"\n",
    "                                              , sep=\"\"))\n",
    "\n",
    "rxExecuteSQLDDL(outOdbcDS, sSQLString = paste(\"SELECT * INTO Account_Info_Sort FROM Account_Info\n",
    "                                              ORDER BY accountID, recordDateTime desc;\"\n",
    "                                              , sep=\"\"))\n",
    "\n",
    "print(\"Account_Info table sorted in ascending order of accountID, and descending order of recordDateTime.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join of the 2 tables Untagged_Transacations and Account_Info_Sort.\n",
    "# Note: SQL queries are used here becasue the rxMerge function is not available for SQL data sources.\n",
    "## the top 1 is the maximum transactionDateTime up to current transactionDateTime\n",
    "\n",
    "rxExecuteSQLDDL(outOdbcDS, sSQLString = paste(\"DROP TABLE if exists Untagged_Transactions_Account;\"\n",
    "                                              , sep=\"\"))\n",
    "\n",
    "rxExecuteSQLDDL(outOdbcDS, sSQLString = paste(\n",
    "  \"SELECT t1.*, t2.accountOwnerName, t2.accountAddress, t2.accountPostalCode, t2.accountCity, t2.accountState,\n",
    "  t2.accountCountry, t2.accountOpenDate, t2.accountAge, t2.isUserRegistered, \n",
    "  t2.paymentInstrumentAgeInAccount, t2.numPaymentRejects1dPerUser\n",
    "  INTO Untagged_Transactions_Account\n",
    "  FROM \n",
    "  (SELECT * FROM Untagged_Transactions) AS t1\n",
    "  OUTER APPLY\n",
    "  (SELECT top 1 * FROM Account_Info_Sort AS t WHERE t.accountID = t1.accountID and t.recordDateTime <= t1.transactionDateTime) AS t2\n",
    "  WHERE t1.accountID = t2.accountID;\"\n",
    "  , sep=\"\"))\n",
    "\n",
    "\n",
    "print(\"Merging of the two tables completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove duplicates based on keys: transactionID, accountID, transactionDateTime, transactionAmount.\n",
    "## Sometimes an entire transaction might be divided into multiple sub-transactions, so we can have the same IDs and Time but different amounts. \n",
    "## Note that it will be done with SQL queries and not with rx functions because evaluating if a row is a duplicate would not be possible \n",
    "## if the data is loaded chunk by chunk.\n",
    "\n",
    "rxExecuteSQLDDL(outOdbcDS, sSQLString = paste(\n",
    "  \"WITH cte_1\n",
    "  AS (SELECT ROW_NUMBER() OVER (PARTITION BY transactionID, accountID, transactionDateTime, transactionAmount ORDER BY transactionID ASC) RN \n",
    "  FROM Untagged_Transactions_Account)\n",
    "  DELETE FROM cte_1\n",
    "  WHERE  RN > 1;\"\n",
    "  , sep=\"\"))\n",
    "\n",
    "rxExecuteSQLDDL(outOdbcDS, sSQLString = paste(\n",
    "  \"WITH cte_2\n",
    "  AS (SELECT ROW_NUMBER() OVER (PARTITION BY transactionID, accountID, transactionDateTime, transactionAmount ORDER BY transactionID ASC) RN \n",
    "  FROM Fraud_Transactions)\n",
    "  DELETE FROM cte_2\n",
    "  WHERE  RN > 1;\"\n",
    "  , sep=\"\"))\n",
    "\n",
    "\n",
    "print(\"Duplicates removed from Untagged_Transactions_Account and Fraud_Transactions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We aggregate the Fraud table on the account level and create start and end date time.\n",
    "# We then perform a left join with the Untagged_Transactions_Account table. \n",
    "# This gives us a table with all the previous data, in addition to the date time of the 1st and last transactions for the accounts \n",
    "# that were in the fraud table. \n",
    "\n",
    "Untagged_Fraud_Account_sql <- RxSqlServerData(sqlQuery = \n",
    "                                                \"SELECT t1.*, t2.startDateNTime, t2.endDateNTime\n",
    "                                                FROM Untagged_Transactions_Account AS t1\n",
    "                                                LEFT JOIN\n",
    "                                                (SELECT accountID, min(transactionDateTime) as startDateNTime, max(transactionDateTime) as endDateNTime \n",
    "                                                FROM Fraud_Transactions\n",
    "                                                GROUP BY accountID) AS t2\n",
    "                                                ON t1.accountID = t2.accountID\",\n",
    "                                              connectionString = connection_string)\n",
    "\n",
    "# Output table pointer.\n",
    "Tagged_sql <- RxSqlServerData(table = \"Tagged\", connectionString = connection_string)\n",
    "\n",
    "# We create the label variable as follows:\n",
    "## if accountID can't be found in the fraud dataset, tag it as 0: not fraudulent.\n",
    "## if accountID is found in the fraud dataset and transactionDateTime is within the fraud time range, tag it as 1: fraud.\n",
    "## if accountID is found in the fraud dataset but transactionDateTime is out of the fraud time range, tag it as 2: pre-fraud.\n",
    "\n",
    "rxDataStep(inData = Untagged_Fraud_Account_sql, \n",
    "           outFile = Tagged_sql,\n",
    "           overwrite = TRUE,\n",
    "           rowsPerRead = 200000,\n",
    "           transforms = list(\n",
    "             label = ifelse(is.na(startDateNTime), 0, \n",
    "                            ifelse(transactionDateTime >= startDateNTime & transactionDateTime <= endDateNTime, 1, 2))\n",
    "              ))\n",
    "\n",
    "\n",
    "print(\"Tags Added to the Data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Splitting and Data Cleaning of the training set\n",
    "\n",
    "In this step, we:\n",
    "\n",
    "**1.** Split the tagged data set into a Training and a Testing set. \n",
    "\n",
    "**2.** Clean the training set and perform some preprocessing.   \n",
    "\n",
    "**Input:** Tagged data set.\n",
    "\n",
    "**Output:** Training and Testing sets, and cleaned Training set Tagged_Training_Processed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the Tagged data set into a Training and a Testing set.\n",
    "\n",
    "## Create the Hash_Id table containing accountID hashed to 100 bins. \n",
    "## The advantage of using a hashing function for splitting is to:\n",
    "## - ensure that the same accountID ends up in the same split.\n",
    "## - permit repeatability of the experiment.  \n",
    "rxExecuteSQLDDL(outOdbcDS, sSQLString = paste(\"DROP TABLE if exists Hash_Id;\", sep = \"\"))\n",
    "\n",
    "rxExecuteSQLDDL(outOdbcDS, sSQLString = paste(\n",
    "  \"SELECT accountID, ABS(CAST(CAST(HashBytes('MD5', accountID) AS VARBINARY(64)) AS BIGINT) % 100) AS hashCode  \n",
    "  INTO Hash_Id\n",
    "  FROM Tagged ;\", sep = \"\"))\n",
    "\n",
    "## Point to the training set. \n",
    "## At the same time, we remove:\n",
    "## - variables not used in the next steps (intermediate variables, variables not needed for the training, variables with only missing values). \n",
    "## - observations with labels equal to 2 (pre-fraud).\n",
    "## - observations where accountID, transactionID and transactionDateTime are missing. \n",
    "## - observations where the transaction amount in USD is negative. \n",
    "\n",
    "query_training <- \"SELECT label, accountID, transactionID, transactionDateTime, isProxyIP, paymentInstrumentType, cardType, paymentBillingAddress,\n",
    "                          paymentBillingPostalCode, paymentBillingCountryCode, paymentBillingName, accountAddress, accountPostalCode,  \n",
    "                          accountCountry, accountOwnerName, shippingAddress, transactionCurrencyCode,localHour, ipState, ipPostCode,\n",
    "                          ipCountryCode, browserLanguage, paymentBillingState, accountState, transactionAmountUSD, digitalItemCount, \n",
    "                          physicalItemCount, accountAge, paymentInstrumentAgeInAccount, numPaymentRejects1dPerUser, isUserRegistered,\n",
    "                          transactionDate, transactionTime\n",
    "                   FROM Tagged \n",
    "                   WHERE accountID IN (SELECT accountID from Hash_Id WHERE hashCode <= 70)\n",
    "                   AND label != 2\n",
    "                   AND accountID IS NOT NULL\n",
    "                   AND transactionID IS NOT NULL \n",
    "                   AND transactionDateTime IS NOT NULL \n",
    "                   AND cast(transactionAmountUSD as float) >= 0\"\n",
    "\n",
    "Tagged_Training_sql <- RxSqlServerData(sqlQuery = query_training, connectionString = connection_string)\n",
    "\n",
    "print(\"Training and testing sets created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess the training set.\n",
    "\n",
    "clean_preprocess <- function(input_data_query, output_sql_name){\n",
    "  \n",
    "  # Detect variables with missing values. \n",
    "  # No missing values in accountID, transactionID and transactionDateTime since we already filtered out missing values in the query above. \n",
    "  # For rxSummary to give correct info on characters, stringsAsFactors = T should be used in the pointer to the SQL Tagged_Training table.\n",
    "  Tagged_Data_sql_stringsfactors <- RxSqlServerData(sqlQuery = input_data_query, connectionString = connection_string, stringsAsFactors = T)\n",
    "  var <- rxGetVarNames(Tagged_Data_sql_stringsfactors)\n",
    "  formula <- as.formula(paste(\"~\", paste(var, collapse = \"+\")))\n",
    "  summary <- rxSummary(formula, Tagged_Data_sql_stringsfactors, byTerm = TRUE)\n",
    "  variables_NA <- summary$sDataFrame[summary$sDataFrame$MissingObs > 0, 1]\n",
    "  variables_NA <- variables_NA[!variables_NA %in% c(\"accountID\", \"transactionID\", \"transactionDateTime\", \"transactionDate\", \"transactionTime\")]\n",
    "  \n",
    "  # If no missing values, we will only preprocess the data. Otherwise, we clean and preprocess. \n",
    "  if(length(variables_NA) == 0){\n",
    "    print(\"No missing values: only preprocessing will be performed.\")\n",
    "  } else{ \n",
    "    print(\"Variables containing missing values are:\")\n",
    "    print(variables_NA)\n",
    "  }\n",
    "  \n",
    "  # Function to replace missing values with 0. It will be wrapped into rxDataStep. \n",
    "  preprocessing <- function(data) {\n",
    "    data <- data.frame(data, stringsAsFactors = F)\n",
    "    \n",
    "    # Replace missing values with 0 except for localHour with -99. \n",
    "    if(length(var_with_NA) > 0){\n",
    "      for(i in 1:length(var_with_NA)){\n",
    "        row_na <- which(is.na(data[, var_with_NA[i]]) == TRUE) \n",
    "        if(var_with_NA[i] == c(\"localHour\")){\n",
    "          data[row_na, var_with_NA[i]] <- \"-99\"\n",
    "        } else{\n",
    "          data[row_na, var_with_NA[i]] <- \"0\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    # Fix some data entries in isUserRegistered, which should be binary.  \n",
    "    row_na <- which(data[, c(\"isUserRegistered\")] %in% as.character(seq(1, 9)))\n",
    "    data[row_na, c(\"isUserRegistered\")] <- \"0\"\n",
    "    \n",
    "    # Convert a few variables to numeric, replacing non-numeric entries with 0. a few other variables to fix some data entries.  \n",
    "    numeric_to_fix <- c(\"accountAge\", \"paymentInstrumentAgeInAccount\", \"numPaymentRejects1dPerUser\", \"transactionAmountUSD\",\n",
    "                        \"digitalItemCount\", \"physicalItemCount\")\n",
    "    for(i in 1:length(numeric_to_fix)){\n",
    "      data[, numeric_to_fix[i]] <- as.numeric(data[, numeric_to_fix[i]])\n",
    "      row_na <- which(is.na(as.numeric(data[, numeric_to_fix[i]])) == TRUE)\n",
    "      data[row_na, numeric_to_fix[i]] <- 0\n",
    "    }\n",
    "    return(data)  \n",
    "  }\n",
    "  \n",
    "  # Input and Output pointers. \n",
    "  Input_sql <- RxSqlServerData(sqlQuery = input_data_query, connectionString = connection_string)\n",
    "  Output_sql <- RxSqlServerData(table =  output_sql_name, connectionString = connection_string)\n",
    "    \n",
    "  # We drop the output if it already exists as a view in case the SQL SP was executed in the same database before. \n",
    "  rxExecuteSQLDDL(outOdbcDS, sSQLString = sprintf(\"IF OBJECT_ID ('%s', 'V') IS NOT NULL DROP VIEW %s ;\", \n",
    "                                                  output_sql_name, output_sql_name))\n",
    "  \n",
    "  # Perform the data cleaning with rxDataStep. \n",
    "  ## To preserve the type of transactionDateTime, we recreate it.\n",
    "  rxDataStep(inData = Input_sql, \n",
    "             outFile = Output_sql, \n",
    "             overwrite = T, \n",
    "             rowsPerRead = 200000,\n",
    "             transformFunc = preprocessing,\n",
    "             transformObjects = list(var_with_NA = variables_NA),\n",
    "             transforms = list(\n",
    "               transactionDateTime = as.character(as.POSIXct(paste(transactionDate, sprintf(\"%06d\", as.numeric(transactionTime)), sep=\"\"), format = \"%Y%m%d %H%M%S\", tz = \"GMT\"))\n",
    "             ))\n",
    "\n",
    "}\n",
    "\n",
    "# Apply the preprocessing and cleaning to the training set. \n",
    "clean_preprocess(input_data_query = query_training, \n",
    "                 output_sql_name = \"Tagged_Training_Processed\")\n",
    "\n",
    "print(\"Training set cleaned and preprocessed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering on the training set\n",
    "\n",
    "In this step, we:\n",
    "\n",
    "**1.** Create the Risk tables. \n",
    "\n",
    "**2.** Assign risk values to the variables in the training set. \n",
    "\n",
    "**3.** Create flags for mismatches between addresses, and flags for high amount transactions. \n",
    "\n",
    "**4.** Create aggregates corresponding to the number and amount of transactions in the past day and 30 days for every transaction per accountID.\n",
    "\n",
    "**5.** Fix variable types for training. \n",
    "\n",
    "**Input:** Cleaned training set Tagged_Training_Processed.\n",
    "\n",
    "**Output:** Training set with new features Tagged_Training_Preprocessed_Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that creates and uploads to SQL a Risk table for 1 variable. \n",
    "create_risk_table <- function(variable_name, data, smooth1, smooth2){\n",
    "  \n",
    "  # Compute the number of frauds and non-frauds for each level of the variable. \n",
    "  formula <- as.formula(paste(\" ~ F(label) :\", variable_name))\n",
    "  Counts_table <- rxCrossTabs(formula = formula, data = data)$counts[[1]]\n",
    "  Counts <- data.frame(x = colnames(Counts_table), fraudCount = Counts_table[\"1\", ], nonFraudCount = Counts_table[\"0\",], row.names = NULL)\n",
    "  colnames(Counts)[1] <- variable_name\n",
    "  \n",
    "  # Compute the smoothed fraud rate of each level of the variable. \n",
    "  Odds <- (Counts$fraudCount + smooth1)/(Counts$nonFraudCount + Counts$fraudCount + smooth2)\n",
    "  # Compute the log of the smoothed odds ratio.\n",
    "  Risk <- log(Odds/(1-Odds))\n",
    "  \n",
    "  # Create the Risk table.\n",
    "  Risk_df <- as.data.frame(cbind(as.character(Counts[, variable_name]), Risk))\n",
    "  colnames(Risk_df) <- c(variable_name, \"risk\")\n",
    "  \n",
    "  # Export it to SQL: Output table pointer for the Risk Table of the specific variable. \n",
    "  rxSetComputeContext('local')\n",
    "  table_name <- paste(\"Risk_\", toupper(substring(variable_name, 1, 1)), substring(variable_name, 2), sep = \"\", collapse = \" \")\n",
    "  Risk_sql <- RxSqlServerData(table = table_name, connectionString = connection_string)\n",
    "  rxDataStep(inData = Risk_df, outFile = Risk_sql, overwrite = TRUE)\n",
    "  \n",
    "  # Set back the compute context to sql.\n",
    "  rxSetComputeContext(sql)\n",
    "  \n",
    "}\n",
    "\n",
    "# Variables for which we create Risk Tables. \n",
    "risk_vars <- c(\"transactionCurrencyCode\", \"localHour\", \"ipState\", \"ipPostCode\",\"ipCountryCode\", \"browserLanguage\",\n",
    "               \"accountPostalCode\", \"accountState\", \"accountCountry\", \"paymentBillingPostalCode\", \"paymentBillingState\",\n",
    "               \"paymentBillingCountryCode\")\n",
    "\n",
    "# Pointer to the preprocessed training set with stringsAsFactors = TRUE for correct summary computations. \n",
    "Train_sqlstringsfactors <- RxSqlServerData(table = \"Tagged_Training_Processed\", connectionString = connection_string, stringsAsFactors = T)\n",
    "\n",
    "# We apply create_risk_table sequentially over the variables in risk_vars. \n",
    "for(variable_name in risk_vars){\n",
    "  create_risk_table(variable_name = variable_name, data = Train_sqlstringsfactors, smooth1 = 10, smooth2 = 100)\n",
    "}\n",
    "\n",
    "print(\"Risk Tables created using the Training set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign risk values to the variables, and create address mismatch and high amount flags.\n",
    "\n",
    "assign_risk_and_flags <- function(input_sql_name, output_sql_name){\n",
    "  \n",
    "  # Function to assign the risk values. It will be wrapped into rxDataStep. \n",
    "  assign_risk <- function(data) {\n",
    "    data <- data.frame(data, stringsAsFactors = F)\n",
    "    \n",
    "    for(name in  risk_variables){\n",
    "      # Import the Risk table from SQL Server. \n",
    "      table_name <- paste(\"Risk_\", toupper(substring(name, 1, 1)), substring(name, 2), sep = \"\", collapse = \" \")\n",
    "      Risk_sql <- RxSqlServerData(table = table_name, connectionString = connection_string)\n",
    "      Risk_df <- rxImport(Risk_sql)\n",
    "      \n",
    "      # Perform a left outer join with the Risk table. This will assign the risk value to every level of the variable. \n",
    "      data <- base::merge(data, Risk_df, by = name, all.x = TRUE)\n",
    "      new_name <- paste(name, \"Risk\", sep =\"\")\n",
    "      colnames(data)[ncol(data)] <- new_name\n",
    "      \n",
    "      # If a new level was found in the data, the assigned risk is NULL. We convert it to 0. \n",
    "      row_na <- which(is.na(data[, new_name]) == TRUE) \n",
    "      data[row_na, new_name] <- 0\n",
    "      \n",
    "    }  \n",
    "    return(data)  \n",
    "  }\n",
    "  \n",
    "  \n",
    "  # Input and Output pointers. \n",
    "  Input_sql <- RxSqlServerData(table = input_sql_name, connectionString = connection_string)\n",
    "  Output_sql <- RxSqlServerData(table = output_sql_name, connectionString = connection_string)\n",
    "    \n",
    "  # We drop the output if it already exists as a view in case the SQL SP was executed in the same database before. \n",
    "  rxExecuteSQLDDL(outOdbcDS, sSQLString = sprintf(\"IF OBJECT_ID ('%s', 'V') IS NOT NULL DROP VIEW %s ;\", \n",
    "                                                  output_sql_name, output_sql_name))\n",
    "  \n",
    "  # Create buckets for various numeric variables with the function Bucketize. \n",
    "  # At the same time, we create other variables:\n",
    "  ## isHighAmount: flag for transactions of a high amount. \n",
    "  ## various flags showing if there is a mismatch in the addresses variables.\n",
    "  rxDataStep(inData = Input_sql,\n",
    "             outFile = Output_sql, \n",
    "             overwrite = TRUE, \n",
    "             rowsPerRead = 200000,\n",
    "             transformFunc = assign_risk,\n",
    "             transformObjects =  list(risk_variables = risk_vars, connection_string = connection_string),\n",
    "             transforms = list(\n",
    "               isHighAmount = ifelse(transactionAmountUSD > 150, \"1\", \"0\"),\n",
    "               acctBillingAddressMismatchFlag = ifelse(paymentBillingAddress == accountAddress, \"0\", \"1\"),\n",
    "               acctBillingPostalCodeMismatchFlag = ifelse(paymentBillingPostalCode == accountPostalCode, \"0\", \"1\"),\n",
    "               acctBillingCountryMismatchFlag = ifelse(paymentBillingCountryCode == accountCountry, \"0\", \"1\"),\n",
    "               acctBillingNameMismatchFlag= ifelse(paymentBillingName == accountOwnerName, \"0\", \"1\"),\n",
    "               acctShippingAddressMismatchFlag = ifelse(shippingAddress == accountAddress, \"0\", \"1\"),\n",
    "               shippingBillingAddressMismatchFlag = ifelse(shippingAddress == paymentBillingAddress, \"0\", \"1\")\n",
    "             ))\n",
    "}\n",
    "\n",
    "# Apply the assign_risk_and_flags function. \n",
    "assign_risk_and_flags(input_sql_name = \"Tagged_Training_Processed\",\n",
    "                      output_sql_name = \"Tagged_Training_Processed_Features1\")\n",
    "\n",
    "print(\"Risk values assigned to the variables, and address mismatch and high amount flags created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of transactions and their amounts in the past day and 30 days for every transaction per accountID.\n",
    "\n",
    "compute_aggregates <- function(input_sql_name, output_sql_name){\n",
    "  \n",
    "  # We drop the output if it already exists as a view in case the SQL SP was executed in the same database before. \n",
    "  rxExecuteSQLDDL(outOdbcDS, sSQLString = sprintf(\"IF OBJECT_ID ('%s', 'V') IS NOT NULL DROP VIEW %s ;\", \n",
    "                                                  output_sql_name, output_sql_name))  \n",
    "    \n",
    "  # Import the data set and fix the datetime type. \n",
    "  rxSetComputeContext('local')\n",
    "  \n",
    "  Input_sql <- RxSqlServerData(table = input_sql_name, connectionString = connection_string)\n",
    "  data = rxImport(Input_sql)\n",
    "  data$transactionDateTime <- as.POSIXct(paste(data$transactionDate, sprintf(\"%06d\", as.numeric(data$transactionTime)), sep=\"\"), format = \"%Y%m%d %H%M%S\", tz = \"GMT\")\n",
    "  \n",
    "  # Function that computes the aggregates for a given accountID. \n",
    "  \n",
    "  aggregates_account_level <- function(dt){\n",
    "    if(nrow(dt) == 1){ #if there is only 1 transaction in that account, no aggregation. \n",
    "      return(NULL)\n",
    "      \n",
    "    } else{ \n",
    "      # Perform a cross-apply and filter: for each transactionID, z has data about the other transactionID that occured in the past 30 days.\n",
    "      z = merge(x = dt, y = dt[, c(\"transactionID\", \"transactionDateTime\", \"transactionAmountUSD\")], by = NULL)\n",
    "      z = z[z$transactionID.x != z$transactionID.y & difftime(z$transactionDateTime.x , z$transactionDateTime.y, units = \"days\")  > 0 & difftime(z$transactionDateTime.x , z$transactionDateTime.y, units = \"days\") < 30,]\n",
    "      \n",
    "      # Keep the transactionIDs that occurred in the past 1 day and 30 days respectively. \n",
    "      z1day = z[difftime(z$transactionDateTime.x , z$transactionDateTime.y, units = \"days\") <= 1, ]\n",
    "      z30day = z[difftime(z$transactionDateTime.x , z$transactionDateTime.y, units = \"days\") <= 30, ]\n",
    "      \n",
    "      # Compute the number of rows (sumPurchaseCount1dPerUser) and the total amount spent in the past day (sumPurchaseAmount1dPerUser). \n",
    "      if(nrow(z30day) == 0){\n",
    "        return(NULL)\n",
    "      } else{\n",
    "        aggsum30day <- aggregate(z30day$transactionAmountUSD.y, by = list(z30day$transactionID.x), FUN = sum)\n",
    "        colnames(aggsum30day) <- c(\"transactionID\", \"sumPurchaseAmount30dPerUser\")\n",
    "        aggcount30day <- aggregate(z30day$transactionAmountUSD.y, by = list(z30day$transactionID.x), FUN = NROW)\n",
    "        colnames(aggcount30day) <- c(\"transactionID\", \"sumPurchaseCount30dPerUser\")\n",
    "        agg30day <- merge(x = aggsum30day, y = aggcount30day  , by = \"transactionID\")\n",
    "      }\n",
    "      \n",
    "      # Compute the number of rows (sumPurchaseCount30dPerUser) and the total amount spent in the past 30 days (sumPurchaseAmount30dPerUser). \n",
    "      if(nrow(z1day) == 0){\n",
    "        agg30day$sumPurchaseAmount1dPerUser <- 0\n",
    "        agg30day$sumPurchaseCount1dPerUser <- 0\n",
    "        return(agg30day)\n",
    "      } else{\n",
    "        aggsum1day <- aggregate(z1day$transactionAmountUSD.y, by = list(z1day$transactionID.x), FUN = sum)\n",
    "        colnames(aggsum1day) <- c(\"transactionID\", \"sumPurchaseAmount1dPerUser\")\n",
    "        aggcount1day <- aggregate(z1day$transactionAmountUSD.y, by = list(z1day$transactionID.x), FUN = NROW)\n",
    "        colnames(aggcount1day) <- c(\"transactionID\", \"sumPurchaseCount1dPerUser\")\n",
    "        agg1day <- merge(x = aggsum1day, y = aggcount1day  , by = \"transactionID\")\n",
    "      }\n",
    "      \n",
    "      # Return the 4 new variables for each transactionID that had other transactions in the past 30 days. \n",
    "      agg <- merge(x = agg1day, y = agg30day  , by = \"transactionID\", all = TRUE)\n",
    "      return(agg)\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Split the data set by accountID. \n",
    "  Splits <- split(data, f = data$accountID)\n",
    "  \n",
    "  # Compute the aggregations for each accountID with the user defined function aggregates_account_level. \n",
    "  Aggregations_list <- lapply(X = Splits, FUN = aggregates_account_level)\n",
    "  \n",
    "  # Bind the results into 1 data frame. \n",
    "  Aggregations_df <- do.call(\"rbind\", Aggregations_list)\n",
    "  \n",
    "  # Add the new variables to the initial data with a left outer join.  \n",
    "  Output_df <- merge(x = data, y = Aggregations_df, by = \"transactionID\", all.x = TRUE)\n",
    "  \n",
    "  # The transactions that had no other transactions in the 30 day time frame have missing values. We convert them to 0.\n",
    "  for(new_name in c(\"sumPurchaseCount1dPerUser\", \"sumPurchaseCount30dPerUser\", \"sumPurchaseAmount1dPerUser\", \"sumPurchaseAmount30dPerUser\")){\n",
    "    row_na <- which(is.na(Output_df[, new_name]) == TRUE) \n",
    "    Output_df[row_na, new_name] <- 0\n",
    "  }\n",
    "\n",
    "  # Write the result back to SQL. \n",
    "  Output_sql <- RxSqlServerData(table = output_sql_name, connectionString = connection_string)\n",
    "  \n",
    "  rxDataStep(inData = Output_df,\n",
    "             outFile = Output_sql,\n",
    "             overwrite = TRUE)\n",
    "  \n",
    "  # Set the compute context back to sql. \n",
    "  rxSetComputeContext(sql)\n",
    "  \n",
    "  # Convert the label to character format in SQL Server. \n",
    "  query <- sprintf( \"ALTER TABLE %s ALTER COLUMN label char(1);\", output_sql_name)\n",
    "  rxExecuteSQLDDL(outOdbcDS, sSQLString = query)\n",
    "  \n",
    "}\n",
    "\n",
    "# Apply the function. \n",
    "input_sql_name <- \"Tagged_Training_Processed_Features1\"\n",
    "output_sql_name <- \"Tagged_Training_Processed_Features\"\n",
    "\n",
    "compute_aggregates(input_sql_name, \n",
    "                   output_sql_name)\n",
    "\n",
    "print(\"Number of transactions and their amounts in the past day and 30 days for every transaction per accountID computed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the column information for training and testing. \n",
    "\n",
    "## Point to the training set with new features with stringsAsFactors = TRUE to get the column information. \n",
    "Tagged_Training_Processed_Features_sql <- RxSqlServerData(table = \"Tagged_Training_Processed_Features\", \n",
    "                                                          connectionString = connection_string,\n",
    "                                                          stringsAsFactors = TRUE)\n",
    "\n",
    "## Save the column information.  \n",
    "column_info <- rxCreateColInfo(Tagged_Training_Processed_Features_sql, sortLevels = TRUE)\n",
    "\n",
    "column_info$accountID <- NULL\n",
    "column_info$transactionDate <- NULL\n",
    "column_info$transactionTime <- NULL\n",
    "column_info$transactionDateTime <- NULL\n",
    "\n",
    "## Create a pointer to the training set, ready for training. \n",
    "Tagged_Training_Processed_Features_sql <- RxSqlServerData(table = \"Tagged_Training_Processed_Features\", \n",
    "                                                          connectionString = connection_string,\n",
    "                                                          colInfo = column_info)\n",
    "\n",
    "print(\"Variables information saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training, Preprocessing and Feature Engineering on the test set, Scoring and Evalutating the model\n",
    "\n",
    "In this step we:\n",
    "\n",
    "**1.** Train a boosted tree classification model on the training set and save it to SQL. \n",
    " \n",
    "**2.** Preprocess and perform feature engineering for the testing set. \n",
    "\n",
    "**3.** Score the GBT on the test set.\n",
    "\n",
    "**4.** Evaluate the tested model: ROC, AUC, and fraud level account metrics. \n",
    "\n",
    "**Input:** Featurized training set Tagged_Training_Processed_Features.\n",
    "\n",
    "**Output:** GBT Model, Predictions and Evaluation Metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the formula after removing variables not used in the modeling.\n",
    "## We remove the id variables, the dates, and the variables for which we computed the risk values. \n",
    "variables_all <- rxGetVarNames(Tagged_Training_Processed_Features_sql)\n",
    "\n",
    "risk_vars <- c(\"transactionCurrencyCode\", \"localHour\", \"ipState\", \"ipPostCode\",\"ipCountryCode\", \"browserLanguage\",\n",
    "               \"accountPostalCode\", \"accountState\", \"accountCountry\", \"paymentBillingPostalCode\", \"paymentBillingState\",\n",
    "               \"paymentBillingCountryCode\")\n",
    "\n",
    "address_vars <- c(\"paymentBillingAddress\", \"accountAddress\", \"shippingAddress\", \"paymentBillingName\", \"accountOwnerName\")\n",
    "\n",
    "variables_to_remove <- c(\"label\", \"accountID\", \"transactionID\", \"transactionDateTime\", \"transactionDate\", \"transactionTime\", risk_vars, address_vars)\n",
    "training_variables <- variables_all[!(variables_all %in% c(\"label\", variables_to_remove))]\n",
    "formula <- as.formula(paste(\"label ~\", paste(training_variables, collapse = \"+\")))\n",
    "\n",
    "print(\"Formula written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the GBT Boosted Trees model.\n",
    "## The unbalancedSets = TRUE argument helps deal with the class imbalance between fraud and non-fraud observations.\n",
    "library(MicrosoftML)\n",
    "boosted_fit <- rxFastTrees(formula = formula,\n",
    "                           data = Tagged_Training_Processed_Features_sql,\n",
    "                           type = c(\"binary\"),\n",
    "                           numTrees = 100,\n",
    "                           learningRate = 0.2,\n",
    "                           splitFraction = 5/24,\n",
    "                           featureFraction = 1,\n",
    "                           minSplit = 10,\n",
    "                           unbalancedSets = TRUE,\n",
    "                           randomSeed = 5)\t\n",
    "\n",
    "# The standard RevoScaleR rxBTrees function can also be used.\n",
    "#boosted_fit <- rxBTrees(formula = formula,\n",
    "#                        data = Tagged_Training_Processed_Features_sql,\n",
    "#                        learningRate = 0.2,\n",
    "#                        minSplit = 10,\n",
    "#                        minBucket = 10,\n",
    "#                        nTree = 100,\n",
    "#                        seed = 5,\n",
    "#                        lossFunction = \"bernoulli\")\n",
    "\n",
    "print(\"Training GBT done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fitted model to SQL Server. Compute Context is set to local. \n",
    "rxSetComputeContext('local')\n",
    "\n",
    "## Open an Odbc connection with SQL Server.\n",
    "OdbcModel <- RxOdbcData(table = \"Trained_Model\", connectionString = connection_string)\n",
    "rxOpen(OdbcModel, \"w\")\n",
    "\n",
    "## Drop the Model table if it exists. \n",
    "if(rxSqlServerTableExists(OdbcModel@table, OdbcModel@connectionString)) {\n",
    "  rxSqlServerDropTable(OdbcModel@table, OdbcModel@connectionString)\n",
    "}\n",
    "\n",
    "## Create an empty Model table. \n",
    "rxExecuteSQLDDL(OdbcModel, \n",
    "                sSQLString = paste(\" CREATE TABLE [\", OdbcModel@table, \"] (\",\n",
    "                                   \"     [id] varchar(200) not null, \",\n",
    "                                   \"     [value] varbinary(max), \",\n",
    "                                   \"     constraint unique_id3 unique (id))\",\n",
    "                                   sep = \"\")\n",
    ")\n",
    "\n",
    "## Write the model to SQL. \n",
    "rxWriteObject(OdbcModel, \"Gradient Boosted Tree\", boosted_fit)\n",
    "\n",
    "# Close the Obdc connection used. \n",
    "rxClose(OdbcModel)\n",
    "\n",
    "# Set the compute context back to SQL. \n",
    "rxSetComputeContext(sql)\n",
    "\n",
    "print(\"Model uploaded to SQL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and perform feature engineering on the testing set, by calling previously defined functions. \n",
    "\n",
    "## Point to the testing set.\n",
    "query_testing <- \"SELECT label, accountID, transactionID, transactionDateTime, isProxyIP, paymentInstrumentType, cardType, paymentBillingAddress,\n",
    "                         paymentBillingPostalCode, paymentBillingCountryCode, paymentBillingName, accountAddress, accountPostalCode,  \n",
    "                         accountCountry, accountOwnerName, shippingAddress, transactionCurrencyCode,localHour, ipState, ipPostCode,\n",
    "                         ipCountryCode, browserLanguage, paymentBillingState, accountState, transactionAmountUSD, digitalItemCount, \n",
    "                         physicalItemCount, accountAge, paymentInstrumentAgeInAccount, numPaymentRejects1dPerUser, isUserRegistered,\n",
    "                         transactionDate, transactionTime\n",
    "                  FROM Tagged \n",
    "                  WHERE accountID IN (SELECT accountID from Hash_Id WHERE hashCode > 70)\n",
    "                  AND label != 2\n",
    "                  AND accountID IS NOT NULL\n",
    "                  AND transactionID IS NOT NULL \n",
    "                  AND transactionDateTime IS NOT NULL \n",
    "                  AND cast(transactionAmountUSD as float) >= 0\"\n",
    "\n",
    "## Apply the preprocessing and cleaning ot the training set. \n",
    "clean_preprocess(input_data_query = query_testing, \n",
    "                 output_sql_name = \"Tagged_Testing_Processed\")\n",
    "\n",
    "## Apply the feature engineering on the testing set. \n",
    "assign_risk_and_flags(input_sql_name = \"Tagged_Testing_Processed\",\n",
    "                      output_sql_name = \"Tagged_Testing_Processed_Features1\")\n",
    "\n",
    "compute_aggregates(input_sql_name = \"Tagged_Testing_Processed_Features1\", \n",
    "                   output_sql_name = \"Tagged_Testing_Processed_Features\")\n",
    "\n",
    "\n",
    "# Create a pointer to the testing set. \n",
    "Tagged_Testing_Processed_Features_sql <- RxSqlServerData(table = \"Tagged_Testing_Processed_Features\",\n",
    "                                                          connectionString = connection_string,\n",
    "                                                          colInfo = column_info)\n",
    "\n",
    "print(\"Testing set preprocessed, and features added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBT Scoring\n",
    "\n",
    "# Pointer to the SQL table where predictions will be written. \n",
    "Predict_Score1_sql <- RxSqlServerData(table = \"Predict_Score1\", connectionString = connection_string)\n",
    "\n",
    "rxPredict(modelObject = boosted_fit,\n",
    "          data = Tagged_Testing_Processed_Features_sql,\n",
    "          outData = Predict_Score1_sql,\n",
    "          overwrite = T,\n",
    "          extraVarsToWrite = c(\"accountID\", \"transactionID\", \"transactionDate\", \"transactionTime\", \"transactionAmountUSD\", \"label\"))\n",
    "\n",
    "\n",
    "# To preserve the type of transactionDateTime, we recreate it.\n",
    "# We also drop some variables. \n",
    "Predict_Score_sql <- RxSqlServerData(table = \"Predict_Score\", connectionString = connection_string)\n",
    "rxDataStep(inData = Predict_Score1_sql, \n",
    "           outFile = Predict_Score_sql, \n",
    "           overwrite = T, \n",
    "           rowsPerRead = 200000,\n",
    "           transforms = list(\n",
    "             transactionDateTime = as.character(as.POSIXct(paste(transactionDate, sprintf(\"%06d\", as.numeric(transactionTime)), sep=\"\"), format = \"%Y%m%d %H%M%S\", tz = \"GMT\")),\n",
    "             transactionDate = NULL, \n",
    "             transactionTime = NULL,\n",
    "             PredictedLabel = NULL, \n",
    "             Score.1 = NULL,\n",
    "             labelProb = Probability.1, \n",
    "             Probability.1 = NULL\n",
    "           ))\n",
    "\n",
    "print(\"Scoring done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation. \n",
    "\n",
    "# Set the compute context to local. \n",
    "rxSetComputeContext('local')\n",
    "\n",
    "# Import the prediction table and convert label to numeric for correct evaluation. \n",
    "# We sort the predictions data in account_date_time order for the account level evaluation.\n",
    "Predictions_sql <- RxSqlServerData(sqlQuery = \"SELECT accountID, transactionDateTime, transactionAmountUSD, label, labelProb\n",
    "                                   FROM Predict_Score\n",
    "                                   ORDER BY accountID, transactionDateTime\",\n",
    "                                   connectionString = connection_string)\n",
    "\n",
    "# Import the prediction table and convert label to numeric for correct evaluation. \n",
    "Predictions <- rxImport(Predictions_sql)\n",
    "Predictions$label <- as.numeric(as.character(Predictions$label))\n",
    "    \n",
    "print(\"Predictions imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC and compute the AUC. \n",
    "ROC <- rxRoc(actualVarName = \"label\", predVarNames = \"labelProb\", data = Predictions, numBreaks = 1000)\n",
    "AUC <- rxAuc(ROC)\n",
    "plot(ROC, title = \"ROC Curve for GBT\")\n",
    "\n",
    "print(sprintf(\"AUC = %s\", AUC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FRAUD ACCOUNT LEVEL METRICS: Implement account-level performance metrics and transaction-level metrics.\n",
    "# ADR -- Fraud account detection rate\n",
    "# VDR -- Value detection rate. The percentage of values saved.\n",
    "# AFPR -- Account-level false positive ratio.\n",
    "# ROC  -- Transaction-level ROC \n",
    "# $ROC -- Dollar weighted ROC\n",
    "# TFPR -- Transaction level false positive ratio.\n",
    "\n",
    "# sampling rate are taken into consideration to derive performance on original unsampled dataset.\n",
    "# Variable contactPeriod is in the unit of days, indicating the lag before a customer is contacted again. \n",
    "# to verify high-score transactions are legitimate. \n",
    "\n",
    "scr2stat <- function(data, contactPeriod, sampleRateNF, sampleRateFrd)\n",
    "{\n",
    "  #scr quantization/binning into 1000 equal bins\n",
    "  \n",
    "  #account level score is the maximum of trans scores of that account\n",
    "  #all transactions after the first fraud transaction detected are value savings\n",
    "  #input score file needs to be acct-date-time sorted   \n",
    "  \n",
    "  nRows <- nrow(data)\n",
    "  nBins <- 1000\n",
    "  \n",
    "  #1. Calculate the perf stats by score band.  \n",
    "  prev_acct <- data$accountID[1]\n",
    "  prev_score <- 0\n",
    "  is_frd_acct <- 0\n",
    "  max_scr <- 0\n",
    "  \n",
    "  scr_hash <- matrix(0, nBins, 10)\n",
    "  \n",
    "  f_scr_rec <- vector(\"numeric\", nBins)\n",
    "  # nf_scr_rec <- matrix(0, nBins, 2)  #count, datetime\n",
    "  nf_scr_rec_count <- vector(\"numeric\", nBins)\n",
    "  nf_scr_rec_time <- vector(\"numeric\", nBins)\n",
    "  \n",
    "  for (r in 1:nRows){\n",
    "    acct <- as.character(data$accountID[r])\n",
    "    dolamt <- data$transactionAmountUSD[r]\n",
    "    label <- data$label[r]\n",
    "    score <- data$labelProb[r]\n",
    "    datetime <- data$transactionDateTime[r]\n",
    "    \n",
    "    if(score == 0){ \n",
    "      score <- score + 0.00001\n",
    "      print (\"The following account has zero score!\")\n",
    "      print (paste(acct, dolamt, datetime,sep = \" \"))\n",
    "    }\n",
    "    \n",
    "    if (acct != prev_acct){\n",
    "      scr_bin <- ceiling(max_scr*nBins)\n",
    "      \n",
    "      if (is_frd_acct){\n",
    "        scr_hash[, 5] <- scr_hash[, 5] + f_scr_rec   #vdr\n",
    "        scr_hash[scr_bin, 1] <- scr_hash[scr_bin, 1] + 1   #adr\n",
    "      } else{\n",
    "        scr_hash[,6] <- scr_hash[, 6] + as.numeric(nf_scr_rec_count)  #FP with contact period, a FP could be considered as multiple\n",
    "        scr_hash[scr_bin, 2] <- scr_hash[scr_bin, 2] + 1   #a FP account considered one acct  \n",
    "      }\n",
    "      \n",
    "      f_scr_rec <- vector(\"numeric\", nBins)\n",
    "      \n",
    "      nf_scr_rec_count <- vector(\"numeric\", nBins)\n",
    "      nf_scr_rec_time <- vector(\"numeric\", nBins)\n",
    "      \n",
    "      is_frd_acct <- 0\n",
    "      total_nf_dol <- 0\n",
    "      total_frd_dol <- 0\n",
    "      max_scr <- 0\n",
    "    }\n",
    "    \n",
    "    if (score > max_scr) {\n",
    "      max_scr <- score\n",
    "    }\n",
    "    \n",
    "    # Find out the bin the current account falls in. \n",
    "    tran_scr_bin <- ceiling(score*nBins)\n",
    "    \n",
    "    \n",
    "    # Dollar weighted ROC and regular ROC.\n",
    "    if(label == 1){\n",
    "      scr_hash[tran_scr_bin, 3] <- scr_hash[tran_scr_bin, 3] + dolamt\n",
    "      scr_hash[tran_scr_bin, 7] <- scr_hash[tran_scr_bin, 7] + 1\n",
    "      is_frd_acct = 1\n",
    "    } else{\n",
    "      scr_hash[tran_scr_bin, 4] <- scr_hash[tran_scr_bin, 4] + dolamt\n",
    "      scr_hash[tran_scr_bin, 8] <- scr_hash[tran_scr_bin, 8] + 1 \n",
    "    }\n",
    "    \n",
    "    # ADR/VDR\n",
    "    if(label == 1){\n",
    "      # ADR\n",
    "      f_scr_rec[tran_scr_bin] <- 1\n",
    "      \n",
    "      # VDR\n",
    "      # If a higher score appeared before the current score, then this is also savings for the higher score.\n",
    "      # Once a fraud transaction is discovered, all subsequent approved transactons are savings.\n",
    "      for(i in  1: ceiling(max_scr*nBins)){\n",
    "        f_scr_rec[i] <- f_scr_rec[i] + dolamt\n",
    "      }\n",
    "    } else { \n",
    "      # False Positive Accounts (FP) with recontact period.\n",
    "      # Check if there is any earlier dates for the same or lower score.\n",
    "      # Update the count and dates when within recontact period.\n",
    "      \n",
    "      #for(i in  1: floor(max_scr*nBins))\n",
    "      for(i in  1: tran_scr_bin)\n",
    "      {\n",
    "        prev_time <- nf_scr_rec_time[i]\n",
    "        #print(paste(i, tran_scr_bin, sep=\" \"))\n",
    "        #print(paste(acct, datetime, sep=\" \"))\n",
    "        #print(prev_time)\n",
    "        if(prev_time > 0){\n",
    "          timeDiff <- difftime(strptime(datetime,\"%Y-%m-%d %H:%M:%S\"), strptime(prev_time, \"%Y-%m-%d %H:%M:%S\"), units = \"days\") \n",
    "          if(timeDiff >= contactPeriod){\n",
    "            nf_scr_rec_count[i] <- nf_scr_rec_count[i] + 1\n",
    "            nf_scr_rec_time[i] <- datetime\n",
    "          }\n",
    "        } else{\n",
    "          nf_scr_rec_count[i] <- nf_scr_rec_count[i] + 1\n",
    "          nf_scr_rec_time[i] <- datetime\n",
    "        }\n",
    "      }\n",
    "    } \n",
    "    prev_acct <- acct;\n",
    "    \n",
    "  }\n",
    "  #1 -- #Frd Acct\n",
    "  #2 -- #NF  Acct with infinite recontact period\n",
    "  #3 -- $Frd Tran\n",
    "  #4 -- $NF  Tran\n",
    "  #5 -- $Frd Saving\n",
    "  #6 -- #NF Acct with finite recontact period\n",
    "  #7 -- #Frd Tran\n",
    "  #8 -- #NF Tran\n",
    "  #9 -- AFPR\n",
    "  #10 --TFPR\n",
    "  \n",
    "  #2. Calculate the cumulative perf counts.\n",
    "  \n",
    "  # 5, 6 already in cumulative during previous calculation.\n",
    "  for (i in (nBins-1):1){\n",
    "    for(j in c(1:4,7:8)){\n",
    "      scr_hash[i, j] <- scr_hash[i, j]+scr_hash[i + 1, j]\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  #3 Calculate AFPR, TFPR:\n",
    "  scr_hash[, 9] <- scr_hash[, 6]/(scr_hash[, 1] + 0.0001)\n",
    "  scr_hash[, 10] <- scr_hash[, 8]/(scr_hash[, 7] + 0.0001)\n",
    "  \n",
    "  #print(scr_hash)\n",
    "  \n",
    "  #4. Calculate the ADR/VDR, ROC percentage.\n",
    "  for(j in c(1:5,7:8)){\n",
    "    scr_hash[, j] <- scr_hash[, j]/scr_hash[1, j]\n",
    "  }\n",
    "  \n",
    "  #5. Adjust for the sampling rate.\n",
    "  for (j in c(1, 3, 5 ,7)){\n",
    "    scr_hash[, j] <- scr_hash[, j]/sampleRateFrd\n",
    "  }\n",
    "  \n",
    "  for (j in c(2, 4, 6 ,8)){\n",
    "    scr_hash[, j] <- scr_hash[, j]/sampleRateNF\n",
    "  }\n",
    "  \n",
    "  for (j in c(9, 10)){\n",
    "    scr_hash[, j] <- scr_hash[, j]/sampleRateNF*sampleRateFrd\n",
    "  }\n",
    "  \n",
    "  perf.df <- as.data.frame(scr_hash)\n",
    "  colnames(perf.df) <- c(\"ADR\", \"PCT NF Acct\", \"Dol Frd\", \"Dol NF\", \"VDR\", \"Acct FP(recontact period)\", \"PCT Frd\", \"PCT NF\",\"AFPR\",\"TFPR\")\n",
    "  return(perf.df)\n",
    "}\n",
    "\n",
    "# Apply the evaluation function to the imported predictions table.\n",
    "  perf <- scr2stat(data = Predictions,\n",
    "                   contactPeriod = 30, \n",
    "                   sampleRateNF = 1,\n",
    "                   sampleRateFrd = 1)\n",
    "\n",
    "print(\"Fraud account level metrics computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud account level metrics plots. \n",
    "\n",
    "## ADR -- Fraud account detection rate\n",
    "plot(perf[, 9], perf[, 1], type = 'b', xlab = 'AFPR', ylab = 'ADR', xlim = c(0, 100))\n",
    "grid()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VDR -- Value detection rate. The percentage of values saved.\n",
    "plot(perf[, 9], perf[, 5], type = 'b', xlab = 'AFPR', ylab = 'VDR', xlim = c(0, 100))\n",
    "grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dollar weighted ROC\n",
    "plot(perf[, 4], perf[, 3], type = 'b', xlab = 'PCT NF Dol', ylab = 'PCT Frd Dol', xlim = c(0, 0.1))\n",
    "grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC\n",
    "plot(perf[, 8], perf[, 7], type = 'b', xlab ='PCT NF', ylab = 'PCT Frd', xlim = c(0, 0.1))\n",
    "grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TFPR vs TDR\n",
    "plot(perf[, 10], perf[, 7], type = 'b', xlab ='TFPR', ylab = 'PCT Frd', xlim = c(0, 100))\n",
    "grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the Obdc connection used for rxExecuteSQLddl functions.\n",
    "rxClose(outOdbcDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
